{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "oak-T6DWu-SL",
        "colab_type": "code",
        "outputId": "fca5978d-1022-494e-a90e-878eaeb9071a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "import os, re\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from string import punctuation\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from nltk import sent_tokenize\n",
        "punctuation += \"«»—…“”\"\n",
        "punct = set(punctuation)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t59s2YT3u4oW",
        "colab_type": "code",
        "outputId": "15438371-cf60-43e0-d194-0222efa3743b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 634
        }
      },
      "source": [
        "!wget https://raw.githubusercontent.com/mannefedov/compling_nlp_hse_course/master/data/sents_with_mistakes.txt\n",
        "!wget https://raw.githubusercontent.com/mannefedov/compling_nlp_hse_course/master/data/correct_sents.txt\n",
        "!wget https://raw.githubusercontent.com/mannefedov/compling_nlp_hse_course/master/data/corpus_500.txt"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-11-22 21:21:35--  https://raw.githubusercontent.com/mannefedov/compling_nlp_hse_course/master/data/sents_with_mistakes.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 123167 (120K) [text/plain]\n",
            "Saving to: ‘sents_with_mistakes.txt’\n",
            "\n",
            "\rsents_with_mistakes   0%[                    ]       0  --.-KB/s               \rsents_with_mistakes 100%[===================>] 120.28K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2019-11-22 21:21:36 (5.69 MB/s) - ‘sents_with_mistakes.txt’ saved [123167/123167]\n",
            "\n",
            "--2019-11-22 21:21:37--  https://raw.githubusercontent.com/mannefedov/compling_nlp_hse_course/master/data/correct_sents.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 120672 (118K) [text/plain]\n",
            "Saving to: ‘correct_sents.txt’\n",
            "\n",
            "correct_sents.txt   100%[===================>] 117.84K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2019-11-22 21:21:38 (5.35 MB/s) - ‘correct_sents.txt’ saved [120672/120672]\n",
            "\n",
            "--2019-11-22 21:21:40--  https://raw.githubusercontent.com/mannefedov/compling_nlp_hse_course/master/data/corpus_500.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1181403 (1.1M) [text/plain]\n",
            "Saving to: ‘corpus_500.txt’\n",
            "\n",
            "corpus_500.txt      100%[===================>]   1.13M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2019-11-22 21:21:40 (24.7 MB/s) - ‘corpus_500.txt’ saved [1181403/1181403]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NymULE1Dv1LE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def normalize(text):\n",
        "    \n",
        "    normalized_text = [(word.strip(punctuation)) for word \\\n",
        "                                                            in text.lower().split()]\n",
        "    normalized_text = [word for word in normalized_text if word]\n",
        "    return normalized_text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vHoG_WfUvV4b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus = []\n",
        "for text in open('corpus_500.txt').read().splitlines():\n",
        "    sents = sent_tokenize(text)\n",
        "    norm_sents = [normalize(sent) for sent in sents]\n",
        "    corpus += norm_sents"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-zngv6upv3ve",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab = set()\n",
        "\n",
        "for sent in corpus:\n",
        "    vocab.update(sent)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-xSPCC_wBAQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "WORDS = Counter()\n",
        "for sent in corpus:\n",
        "    WORDS.update(sent)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NgGwCPI-_z6X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "N = sum(WORDS.values())\n",
        "def P(word, N=N): \n",
        "    return WORDS[word] / N "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ocoC6wKAw4bJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def correction(word): \n",
        "    return max(candidates(word), key=P)\n",
        "\n",
        "def candidates(word): \n",
        "  cands = set()\n",
        "  cands2 = set()\n",
        "  if known2([word])!= set():\n",
        "    cands.update(known2([word]))\n",
        "  if  known2(edits1(word)) != set():\n",
        "    cands.update(known2(edits1(word)))\n",
        "  if known(edits2(word)) != set():\n",
        "    cands2.update(known(edits2(word)))\n",
        "  if known2(edits2(word)) != set():\n",
        "    cands2.update(known(edits2(word)))\n",
        "  return (known([word]) or cands or cands2 or [word] )\n",
        "    #Генерируем кандидатов на исправление. Приоритет следующий: 1) есть ли это слово в списке существующих слов из корпуса 2) есть ли оно в объединенном списке кандидатов, включающем:\n",
        "    #вхождения слова в словаре слов с удаленным символом(ССУС), а также вхождение слов с удаленным символом в ССУС 3) вхождение слов с 2-мя удаленными символами  в список cуществующих слов или ССУС\n",
        "    #4) возвращаем слово, если ничего не нашлось'\". Приоритет обусловлен предположением о том, что отсутствие ошибок более вероятно чем одна ошибка, а одна ошибка более вероятна, чем две и т.д.\n",
        "\n",
        "\n",
        "def known(words): \n",
        "    return set(w for w in words if (w in vocab))\n",
        "\n",
        "def known2 (words):\n",
        "  words2 = set()\n",
        "  for w in words:\n",
        "    if w in vocab1.keys():\n",
        "        words2.update(vocab1[w])\n",
        "  return words2\n",
        "   #Выбираем слова, которые есть в ССУС\n",
        "\n",
        "def edits1(word):\n",
        "    letters    = 'йцукенгшщзхъфывапролджэячсмитьбюё'\n",
        "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
        "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
        "    return set(deletes)\n",
        "\n",
        "def edits2(word): \n",
        "    return set(e2 for e1 in edits1(word) for e2 in edits1(e1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Niqf8FRe1RRP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab1 = dict()\n",
        "for word in vocab:\n",
        "  for edited_word in edits1(word):\n",
        "    if edited_word not in vocab1.keys():\n",
        "      vocab1[edited_word] = {word}\n",
        "    else:\n",
        "      vocab1[edited_word].add(word)\n",
        "#Создаем ССУС"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mbou45T_5YpO",
        "colab_type": "code",
        "outputId": "545ee548-001c-47b9-a50f-9ef4b40b10ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "correction('ооочень')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'очень'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vgj-e6w-6Mr0",
        "colab_type": "text"
      },
      "source": [
        "Спеллчекер готов, далее проверим его качество"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aCIC7xiX1QrC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bad = open('sents_with_mistakes.txt', encoding='utf8').read().splitlines()\n",
        "true = open('correct_sents.txt', encoding='utf8').read().splitlines()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nwoYB69A1YK1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def align_words(sent_1, sent_2):\n",
        "    tokens_1 = sent_1.lower().split()\n",
        "    tokens_2 = sent_2.lower().split()\n",
        "    \n",
        "    tokens_1 = [re.sub('(^\\W+|\\W+$)', '', token) for token in tokens_1 if (set(token)-punct)]\n",
        "    tokens_2 = [re.sub('(^\\W+|\\W+$)', '', token) for token in tokens_2 if (set(token)-punct)]\n",
        "    \n",
        "    return list(zip(tokens_1, tokens_2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DywKQGwXt67W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "total_mistaken = 0\n",
        "mistaken_fixed = 0\n",
        "\n",
        "total_correct = 0\n",
        "correct_broken = 0\n",
        "cashed = {}\n",
        "for i in range(len(true)):\n",
        "    word_pairs = align_words(true[i], bad[i])\n",
        "    for pair in word_pairs:\n",
        "        predicted = cashed.get(pair[1], correction(pair[1]))\n",
        "        cashed[pair[0]] = predicted\n",
        "        if predicted == pair[0]:\n",
        "            correct += 1\n",
        "        total += 1\n",
        "        if pair[0] == pair[1]:\n",
        "            total_correct += 1\n",
        "            if pair[0] !=  predicted:\n",
        "                correct_broken += 1\n",
        "        else:\n",
        "            total_mistaken += 1\n",
        "            if pair[0] == predicted:\n",
        "                mistaken_fixed += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XdZVfiLc14en",
        "colab_type": "code",
        "outputId": "ae1d0547-2112-445e-d58c-82d125f21a0a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "source": [
        "print(correct/total)\n",
        "print(mistaken_fixed/total_mistaken)\n",
        "print(correct_broken/total_correct)\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.6398601398601399\n",
            "0.24558710667689945\n",
            "0.301137016193867\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "174J3NDXBvsK",
        "colab_type": "text"
      },
      "source": [
        "Триграммная модель"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJoV_dt9Of4v",
        "colab_type": "text"
      },
      "source": [
        "Разбиваем на триграммы корпус новостных текстов"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QYFmgxMdzvYr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus_news = [['<start>', '<start>'] + sent + ['<end>'] for sent in corpus]\n",
        "\n",
        "def ngrammer(tokens, n):\n",
        "    ngrams = []\n",
        "    for i in range(0,len(tokens)-n+1):\n",
        "        ngrams.append(' '.join(tokens[i:i+n]))\n",
        "    return ngrams\n",
        "\n",
        "\n",
        "unigrams = Counter()\n",
        "bigrams = Counter()\n",
        "trigrams = Counter()\n",
        "mistakes = []\n",
        "\n",
        "for sentence in corpus_news:\n",
        "    unigrams.update(sentence)\n",
        "    bigrams.update(ngrammer(sentence, 2))\n",
        "    trigrams.update(ngrammer(sentence, 3))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eC9IOKJTOmdC",
        "colab_type": "text"
      },
      "source": [
        "Также разбиваем на триграммы предложения для проверки, т.к. будут анализироваться триграммы (два слова + данное слово для исправления) именно в этих предложениях и сравниваться с триграммами в корпусе новостных текстов. При наличии аналогичных триграммов в корпусе, их частотность будет влиять на вероятность выбора того или иного слова-кандидата.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7gt1tlwEzGIp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sent_tokenizer(text):\n",
        "  corpus = []\n",
        "  for lines in text:\n",
        "    sents = lines.split('\\n')\n",
        "    norm_sents = [normalize(sent) for sent in sents]\n",
        "    corpus += norm_sents\n",
        "  return [['<start>', '<start>'] + sent + ['<end>'] for sent in corpus]\n",
        "#функция разбиения на предложения по строкам, разбиение на слова внутри предложений + добавление '<start>', '<start>' и '<end>' для триграммной модели\n",
        "\n",
        "corpus_bad = sent_tokenizer(bad)\n",
        "corpus_true = sent_tokenizer(true)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ejGLOUz2G90z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "total_mistaken = 0\n",
        "mistaken_fixed = 0\n",
        "\n",
        "total_correct = 0\n",
        "correct_broken = 0\n",
        "\n",
        "def correction2(dictionary): \n",
        "    return max(dictionary, key = dictionary.get)\n",
        "#меняем функцию выбора кандидатов так, что на вход подается словарь, где слова - ключи, а вероятности с учетом триграммов - значения.\n",
        "\n",
        "for j in range(len(corpus_bad)):\n",
        "  for i in range(len(corpus_bad[j])):\n",
        "    if corpus_bad[j][i+2] == '<end>':\n",
        "      break\n",
        "    current_trigram = corpus_bad[j][i] + ' ' + corpus_bad[j][i+1] + ' ' + corpus_bad[j][i+2]\n",
        "    current_bigram = corpus_bad[j][i] + ' ' + corpus_bad[j][i+1]\n",
        "    candid_dict = dict()\n",
        "    for candidate in candidates(corpus_bad[j][i+2]):\n",
        "      possible_trigram = corpus_bad[j][i] + ' ' + corpus_bad[j][i+1] + ' ' + candidate\n",
        "      if possible_trigram in trigrams:\n",
        "        Prob = trigrams[possible_trigram]/bigrams[current_bigram]\n",
        "        candid_dict[candidate] = Prob\n",
        "      else:\n",
        "        Prob = WORDS[candidate] / N\n",
        "        candid_dict[candidate] = Prob\n",
        "#если биграмм + слово-кандидат есть в списке триграммов, то это меняет вероятность. Если нет, то вероятность вычисляется как и раньше.\n",
        "    predicted = correction2(candid_dict)\n",
        "    total+=1\n",
        "    if predicted == corpus_true[j][i+2]:\n",
        "      correct += 1\n",
        "    if corpus_true[j][i+2] == corpus_bad[j][i+2]:\n",
        "      total_correct += 1\n",
        "      if corpus_true[j][i+2]  !=  predicted:\n",
        "        correct_broken += 1\n",
        "    else:\n",
        "      total_mistaken += 1\n",
        "      if corpus_true[j][i+2] == predicted:\n",
        "        mistaken_fixed += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nxsMtDvpNZd3",
        "colab_type": "code",
        "outputId": "f47be882-f491-40cd-c813-5199187bed39",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "source": [
        "print(correct/total)\n",
        "print(mistaken_fixed/total_mistaken)\n",
        "print(correct_broken/total_correct)\n",
        "#0.7844155844155845\n",
        "#0.24942440521872603\n",
        "#0.1355231422992994"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7844155844155845\n",
            "0.24942440521872603\n",
            "0.1355231422992994\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-OICmkaCGsP",
        "colab_type": "text"
      },
      "source": [
        "При использовании триграммной модели число случаев, когда спеллчекер неверно исправляет правильные слова, уменьшилось более, чем в два раза. Число верно исправленных ошибок при этом изменилось незначительно. То, что исправляется всего лишь 24-25%, ошибок можно объяснить тем, что корпус маловат и нерепрезентативен для составления полноценного словаря. Если бы тестовые предложения были бы взяты из новостей, то возможно и процент верно найденных ошибок был бы выше."
      ]
    }
  ]
}