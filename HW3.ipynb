{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"HW3.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"oak-T6DWu-SL","colab_type":"code","outputId":"5ce22c95-f68a-4248-f503-8e8ad2f5a7a7","executionInfo":{"status":"ok","timestamp":1574429975359,"user_tz":-180,"elapsed":1202,"user":{"displayName":"Петр Ф","photoUrl":"","userId":"00541756801210435258"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["import os, re\n","import nltk\n","nltk.download('punkt')\n","from string import punctuation\n","import numpy as np\n","from collections import Counter\n","from nltk import sent_tokenize\n","punctuation += \"«»—…“”\"\n","punct = set(punctuation)"],"execution_count":133,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"t59s2YT3u4oW","colab_type":"code","outputId":"9fd428dd-1423-4e71-a3ca-0a49278037f0","executionInfo":{"status":"ok","timestamp":1574429984065,"user_tz":-180,"elapsed":8052,"user":{"displayName":"Петр Ф","photoUrl":"","userId":"00541756801210435258"}},"colab":{"base_uri":"https://localhost:8080/","height":634}},"source":["!wget https://raw.githubusercontent.com/mannefedov/compling_nlp_hse_course/master/data/sents_with_mistakes.txt\n","!wget https://raw.githubusercontent.com/mannefedov/compling_nlp_hse_course/master/data/correct_sents.txt\n","!wget https://raw.githubusercontent.com/mannefedov/compling_nlp_hse_course/master/data/corpus_500.txt"],"execution_count":134,"outputs":[{"output_type":"stream","text":["--2019-11-22 13:39:36--  https://raw.githubusercontent.com/mannefedov/compling_nlp_hse_course/master/data/sents_with_mistakes.txt\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 123167 (120K) [text/plain]\n","Saving to: ‘sents_with_mistakes.txt.1’\n","\n","\r          sents_wit   0%[                    ]       0  --.-KB/s               \rsents_with_mistakes 100%[===================>] 120.28K  --.-KB/s    in 0.01s   \n","\n","2019-11-22 13:39:37 (9.83 MB/s) - ‘sents_with_mistakes.txt.1’ saved [123167/123167]\n","\n","--2019-11-22 13:39:39--  https://raw.githubusercontent.com/mannefedov/compling_nlp_hse_course/master/data/correct_sents.txt\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 120672 (118K) [text/plain]\n","Saving to: ‘correct_sents.txt.1’\n","\n","correct_sents.txt.1 100%[===================>] 117.84K  --.-KB/s    in 0.01s   \n","\n","2019-11-22 13:39:39 (9.80 MB/s) - ‘correct_sents.txt.1’ saved [120672/120672]\n","\n","--2019-11-22 13:39:41--  https://raw.githubusercontent.com/mannefedov/compling_nlp_hse_course/master/data/corpus_500.txt\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 1181403 (1.1M) [text/plain]\n","Saving to: ‘corpus_500.txt.1’\n","\n","corpus_500.txt.1    100%[===================>]   1.13M  --.-KB/s    in 0.02s   \n","\n","2019-11-22 13:39:41 (47.9 MB/s) - ‘corpus_500.txt.1’ saved [1181403/1181403]\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"NymULE1Dv1LE","colab_type":"code","colab":{}},"source":["def normalize(text):\n","    \n","    normalized_text = [(word.strip(punctuation)) for word \\\n","                                                            in text.lower().split()]\n","    normalized_text = [word for word in normalized_text if word]\n","    return normalized_text"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vHoG_WfUvV4b","colab_type":"code","colab":{}},"source":["corpus = []\n","for text in open('corpus_500.txt').read().splitlines():\n","    sents = sent_tokenize(text)\n","    norm_sents = [normalize(sent) for sent in sents]\n","    corpus += norm_sents"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-zngv6upv3ve","colab_type":"code","colab":{}},"source":["vocab = set()\n","\n","for sent in corpus:\n","    vocab.update(sent)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8-xSPCC_wBAQ","colab_type":"code","colab":{}},"source":["WORDS = Counter()\n","for sent in corpus:\n","    WORDS.update(sent)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NgGwCPI-_z6X","colab_type":"code","colab":{}},"source":["N = sum(WORDS.values())\n","def P(word, N=N): \n","    return WORDS[word] / N "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ocoC6wKAw4bJ","colab_type":"code","colab":{}},"source":["def correction(word): \n","    return max(candidates(word), key=P)\n","\n","def candidates(word): \n","  cands = set()\n","  cands2 = set()\n","  if known2([word])!= set():\n","    cands.update(known2([word]))\n","  if  known2(edits1(word)) != set():\n","    cands.update(known2(edits1(word)))\n","  if known(edits2(word)) != set():\n","    cands2.update(known(edits2(word)))\n","  if known2(edits2(word)) != set():\n","    cands2.update(known(edits2(word)))\n","  return (known([word]) or cands or cands2 or [word] )\n","    #Генерируем кандидатов на исправление. Приоритет следующий: 1) есть ли это слово в списке существующих слов из корпуса 2) есть ли оно в объединенном списке кандидатов, включающем:\n","    #вхождения слова в словаре слов с удаленным символом(ССУС), а также вхождение слов с удаленным символом в ССУС 3) вхождение слов с 2-мя удаленными символами  в список cуществующих слов или ССУС\n","    #4) возвращаем слово, если ничего не нашлось'\". Приоритет обусловлен предположением о том, что отсутствие ошибок более вероятно чем одна ошибка, а одна ошибка более вероятна, чем две и т.д.\n","\n","\n","def known(words): \n","    return set(w for w in words if (w in vocab))\n","\n","def known2 (words):\n","  words2 = set()\n","  for w in words:\n","    if w in vocab1.keys():\n","        words2.update(vocab1[w])\n","  return words2\n","   #Выбираем слова, которые есть в ССУС\n","\n","def edits1(word):\n","    letters    = 'йцукенгшщзхъфывапролджэячсмитьбюё'\n","    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n","    deletes    = [L + R[1:]               for L, R in splits if R]\n","    return set(deletes)\n","\n","def edits2(word): \n","    return set(e2 for e1 in edits1(word) for e2 in edits1(e1))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Niqf8FRe1RRP","colab_type":"code","colab":{}},"source":["vocab1 = dict()\n","for word in vocab:\n","  for edited_word in edits1(word):\n","    if edited_word not in vocab1.keys():\n","      vocab1[edited_word] = {word}\n","    else:\n","      vocab1[edited_word].add(word)\n","#Создаем ССУС"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Mbou45T_5YpO","colab_type":"code","outputId":"52a91350-6d96-40fb-9ed3-e72bcf16a04d","executionInfo":{"status":"ok","timestamp":1574424001168,"user_tz":-180,"elapsed":764,"user":{"displayName":"Петр Ф","photoUrl":"","userId":"00541756801210435258"}},"colab":{"base_uri":"https://localhost:8080/","height":36}},"source":["correction('ооочень')"],"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'очень'"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"markdown","metadata":{"id":"Vgj-e6w-6Mr0","colab_type":"text"},"source":["Спеллчекер готов, далее проверим его качество"]},{"cell_type":"code","metadata":{"id":"aCIC7xiX1QrC","colab_type":"code","colab":{}},"source":["bad = open('sents_with_mistakes.txt', encoding='utf8').read().splitlines()\n","true = open('correct_sents.txt', encoding='utf8').read().splitlines()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"nwoYB69A1YK1","colab_type":"code","colab":{}},"source":["def align_words(sent_1, sent_2):\n","    tokens_1 = sent_1.lower().split()\n","    tokens_2 = sent_2.lower().split()\n","    \n","    tokens_1 = [re.sub('(^\\W+|\\W+$)', '', token) for token in tokens_1 if (set(token)-punct)]\n","    tokens_2 = [re.sub('(^\\W+|\\W+$)', '', token) for token in tokens_2 if (set(token)-punct)]\n","    \n","    return list(zip(tokens_1, tokens_2))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"DywKQGwXt67W","colab_type":"code","colab":{}},"source":["correct = 0\n","total = 0\n","\n","total_mistaken = 0\n","mistaken_fixed = 0\n","\n","total_correct = 0\n","correct_broken = 0\n","cashed = {}\n","for i in range(len(true)):\n","    word_pairs = align_words(true[i], bad[i])\n","    for pair in word_pairs:\n","        predicted = cashed.get(pair[1], correction(pair[1]))\n","        cashed[pair[0]] = predicted\n","        if predicted == pair[0]:\n","            correct += 1\n","        total += 1\n","        if pair[0] == pair[1]:\n","            total_correct += 1\n","            if pair[0] !=  predicted:\n","                correct_broken += 1\n","        else:\n","            total_mistaken += 1\n","            if pair[0] == predicted:\n","                mistaken_fixed += 1"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"XdZVfiLc14en","colab_type":"code","outputId":"9d973bbf-5cf9-4940-fba5-ce7b721b6cfd","executionInfo":{"status":"ok","timestamp":1574430131622,"user_tz":-180,"elapsed":1070,"user":{"displayName":"Петр Ф","photoUrl":"","userId":"00541756801210435258"}},"colab":{"base_uri":"https://localhost:8080/","height":73}},"source":["print(correct/total)\n","print(mistaken_fixed/total_mistaken)\n","print(correct_broken/total_correct)\n"],"execution_count":152,"outputs":[{"output_type":"stream","text":["0.6398601398601399\n","0.24405218726016883\n","0.3009073159526818\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"174J3NDXBvsK","colab_type":"text"},"source":["Триграммная модель"]},{"cell_type":"markdown","metadata":{"id":"HJoV_dt9Of4v","colab_type":"text"},"source":["Разбиваем на триграммы корпус новостных текстов"]},{"cell_type":"code","metadata":{"id":"QYFmgxMdzvYr","colab_type":"code","colab":{}},"source":["corpus_news = [['<start>', '<start>'] + sent + ['<end>'] for sent in corpus]\n","\n","def ngrammer(tokens, n):\n","    ngrams = []\n","    for i in range(0,len(tokens)-n+1):\n","        ngrams.append(' '.join(tokens[i:i+n]))\n","    return ngrams\n","\n","\n","unigrams = Counter()\n","bigrams = Counter()\n","trigrams = Counter()\n","mistakes = []\n","\n","for sentence in corpus_news:\n","    unigrams.update(sentence)\n","    bigrams.update(ngrammer(sentence, 2))\n","    trigrams.update(ngrammer(sentence, 3))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eC9IOKJTOmdC","colab_type":"text"},"source":["Также разбиваем на триграммы предложения для проверки, т.к. будут анализироваться триграммы (два слова + данное слово для исправления) именно в этих предложениях и сравниваться с триграммами в корпусе новостных текстов. При наличии аналогичных триграммов в корпусе, их частотность будет влиять на вероятность выбора того или иного слова-кандидата.  "]},{"cell_type":"code","metadata":{"id":"7gt1tlwEzGIp","colab_type":"code","colab":{}},"source":["def sent_tokenizer(text):\n","  corpus = []\n","  for lines in text:\n","    sents = lines.split('\\n')\n","    norm_sents = [normalize(sent) for sent in sents]\n","    corpus += norm_sents\n","  return [['<start>', '<start>'] + sent + ['<end>'] for sent in corpus]\n","#функция разбиения на предложения по строкам, разбиение на слова внутри предложений + добавление '<start>', '<start>' и '<end>' для триграммной модели\n","\n","corpus_bad = sent_tokenizer(bad)\n","corpus_true = sent_tokenizer(true)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ejGLOUz2G90z","colab_type":"code","colab":{}},"source":["correct = 0\n","total = 0\n","\n","total_mistaken = 0\n","mistaken_fixed = 0\n","\n","total_correct = 0\n","correct_broken = 0\n","\n","def correction2(dictionary): \n","    return max(dictionary, key = dictionary.get)\n","#меняем функцию выбора кандидатов так, что на вход подается словарь, где слова - ключи, а вероятности с учетом триграммов - значения.\n","\n","for j in range(len(corpus_bad)):\n","  for i in range(len(corpus_bad[j])):\n","    if corpus_bad[j][i+2] == '<end>':\n","      break\n","    current_trigram = corpus_bad[j][i] + ' ' + corpus_bad[j][i+1] + ' ' + corpus_bad[j][i+2]\n","    current_bigram = corpus_bad[j][i] + ' ' + corpus_bad[j][i+1]\n","    candid_dict = dict()\n","    for candidate in candidates(corpus_bad[j][i+2]):\n","      possible_trigram = corpus_bad[j][i] + ' ' + corpus_bad[j][i+1] + ' ' + candidate\n","      if possible_trigram in trigrams:\n","        Prob = WORDS[candidate] / N + (1 + (bigrams[current_bigram]/trigrams[possible_trigram]))\n","        candid_dict[candidate] = Prob\n","      else:\n","        Prob = WORDS[candidate] / N\n","        candid_dict[candidate] = Prob\n","#если биграмм + слово-кандидат есть в списке триграммов, то это меняет вероятность. Если нет, то вероятность вычисляется как и раньше.\n","    predicted = correction2(candid_dict)\n","    total+=1\n","    if predicted == corpus_true[j][i+2]:\n","      correct += 1\n","    if corpus_true[j][i+2] == corpus_bad[j][i+2]:\n","      total_correct += 1\n","      if corpus_true[j][i+2]  !=  predicted:\n","        correct_broken += 1\n","    else:\n","      total_mistaken += 1\n","      if corpus_true[j][i+2] == predicted:\n","        mistaken_fixed += 1"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"nxsMtDvpNZd3","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":73},"outputId":"ca42898c-2baa-458d-a3cb-326078098871","executionInfo":{"status":"ok","timestamp":1574430368672,"user_tz":-180,"elapsed":1048,"user":{"displayName":"Петр Ф","photoUrl":"","userId":"00541756801210435258"}}},"source":["print(correct/total)\n","print(mistaken_fixed/total_mistaken)\n","print(correct_broken/total_correct)"],"execution_count":156,"outputs":[{"output_type":"stream","text":["0.7837162837162838\n","0.24405218726016883\n","0.1355231422992994\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Q-OICmkaCGsP","colab_type":"text"},"source":["При использовании триграммной модели число случаев, когда спеллчекер неверно исправляет правильные слова, уменьшилось более, чем в два раза. Число верно исправленных ошибок при этом не изменилось. То, что исправляется всего лишь 24%, ошибок можно объяснить тем, что корпус маловат и нерепрезентативен для составления полноценного словаря. Если бы тестовые предложения были бы взяты из новостей, то возможно и процент верно найденных ошибок был бы выше."]}]}