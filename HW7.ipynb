{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW7.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "90J0vrCnPkvI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!pip install pymorphy2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CvTLBbigkrQt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!pip install Cython numpy\n",
        "#!pip install git+https://github.com/lopuhin/python-adagram.git"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qtbi18m2P-ku",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h2morwofCCYk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import adagram\n",
        "from lxml import html\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.corpus import wordnet as wn\n",
        "from pymorphy2 import MorphAnalyzer\n",
        "from string import punctuation\n",
        "import json, os\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "import warnings\n",
        "import zipfile\n",
        "warnings.filterwarnings('ignore')\n",
        "morph = MorphAnalyzer()\n",
        "punct = punctuation+'«»—…“”*№–'\n",
        "stops = set(stopwords.words('russian'))\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "def tokenize(text):\n",
        "    \n",
        "    words = [word.strip(punct) for word in text.lower().split() if word and word not in stops]\n",
        "    words = [word for word in words if word]\n",
        "\n",
        "    return words\n",
        "\n",
        "def normalize(text):\n",
        "    \n",
        "    words = tokenize(text)\n",
        "    words = [morph.parse(word)[0].normal_form for word in words if word]\n",
        "\n",
        "    return words"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9bZJAjlzgcz3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "zip_file = files.upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4v310o8gyV1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "zipcorpus = zipfile.ZipFile('paraphraser.zip')\n",
        "zipcorpus.extractall()\n",
        "zipcorpus.close"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ko0gIYAJ3tPE",
        "colab_type": "text"
      },
      "source": [
        "##Адаграм"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "meaZ_sIHQDdy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus_xml = html.fromstring(open('paraphrases.xml', 'rb').read())\n",
        "texts_1 = []\n",
        "texts_2 = []\n",
        "classes = []\n",
        "\n",
        "for p in corpus_xml.xpath('//paraphrase'):\n",
        "    texts_1.append(p.xpath('./value[@name=\"text_1\"]/text()')[0])\n",
        "    texts_2.append(p.xpath('./value[@name=\"text_2\"]/text()')[0])\n",
        "    classes.append(p.xpath('./value[@name=\"class\"]/text()')[0])\n",
        "    \n",
        "data = pd.DataFrame({'text_1':texts_1, 'text_2':texts_2, 'label':classes})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IiLCrYgijj3f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data['text_1_norm'] = data['text_1'].apply(normalize)\n",
        "data['text_2_norm'] = data['text_2'].apply(normalize)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PvOqTkSxjmbr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!wget 'https://s3.amazonaws.com/kostia.lopuhin/all.a010.p10.d300.w5.m100.nonorm.slim.joblib'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZSiOg6zAlxBh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vm = adagram.VectorModel.load('all.a010.p10.d300.w5.m100.nonorm.slim.joblib')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMRfPpOz9u49",
        "colab_type": "text"
      },
      "source": [
        "Получение целевых слов и контекста с заданным окном (по умолчанию окно -3)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77WfTFjapZIb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "words = [0,1,2,3,4,5,6,7,8,9]\n",
        "def get_words_in_context(words, window=3):\n",
        "    words_in_context = []\n",
        "    for i in range(len(words)):\n",
        "      word = words[i]\n",
        "      before_ind = max(0, i - window) \n",
        "      after_ind = min(len(words), i + window + 1)\n",
        "      before_word = words[before_ind:i]\n",
        "      after_word = words[i+1:after_ind]\n",
        "      context = before_word + after_word\n",
        "      words_in_context += [[word, context]]\n",
        "    return words_in_context"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-ngvt-isYbz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "outputId": "c7e40498-97ed-4f7f-ae15-629e1c44a1f5"
      },
      "source": [
        "get_words_in_context(words)"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[0, [1, 2, 3]],\n",
              " [1, [0, 2, 3, 4]],\n",
              " [2, [0, 1, 3, 4, 5]],\n",
              " [3, [0, 1, 2, 4, 5, 6]],\n",
              " [4, [1, 2, 3, 5, 6, 7]],\n",
              " [5, [2, 3, 4, 6, 7, 8]],\n",
              " [6, [3, 4, 5, 7, 8, 9]],\n",
              " [7, [4, 5, 6, 8, 9]],\n",
              " [8, [5, 6, 7, 9]],\n",
              " [9, [6, 7, 8]]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9hdHJzF99IQ",
        "colab_type": "text"
      },
      "source": [
        "Векторизуем тексты с помощью Адаграма"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wirjjjFanCso",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_embedding_ad(model, text, dim=300, window=3):\n",
        "  words_in_context = get_words_in_context(text, window)\n",
        "  vectors = np.zeros((len(words_in_context), dim))\n",
        "  for i,[word, context] in enumerate(words_in_context):\n",
        "    try:\n",
        "      main_sense = model.disambiguate(word, context).argmax()\n",
        "      v = model.sense_vector(word, main_sense)\n",
        "      vectors[i] = v\n",
        "    except (KeyError, ValueError):\n",
        "      #print('Что-то пошло не так')\n",
        "      continue\n",
        "  if vectors.any():\n",
        "    vector = np.average(vectors, axis=0)\n",
        "  else:\n",
        "    vector = np.zeros((dim))\n",
        "  return vector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c-ik9u5coety",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "get_embedding_ad(vm, ['ломать', 'не', 'строить'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xH46txSml9Dk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text_1_ad = [get_embedding_ad(vm, text) for text in data['text_1_norm']]\n",
        "text_2_ad = [get_embedding_ad(vm, text) for text in data['text_2_norm']]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NKrTieEQ19Rt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_text = np.concatenate([text_1_ad, text_2_ad], axis=1)\n",
        "y = data.label.values\n",
        "clf = LogisticRegression()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XyzSlxxE20zN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "828e6a0b-94fa-4dfd-c070-dcbbf43562bc"
      },
      "source": [
        "np.mean(cross_val_score(clf, X_text, y, scoring='f1_micro', cv=5))"
      ],
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.42451817925119767"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 152
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AiWAVHM5VOFl",
        "colab_type": "text"
      },
      "source": [
        "##Алгоритм Леска\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Yb-RKVUaqph",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMAqtDwP-ZJc",
        "colab_type": "text"
      },
      "source": [
        "Функция для алгоритма Леска"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJfi0uqMT0vs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def lesk(word, sentence):\n",
        "    norm_sent = set(sentence)\n",
        "    synsets =  wn.synsets(word)\n",
        "    intersections = {}\n",
        "    for i, syns in enumerate(synsets):\n",
        "      definition = normalize(syns.definition())\n",
        "      intersections[i] = len(set(definition).intersection(norm_sent))\n",
        "    bestsence = max(intersections, key = intersections.get )\n",
        "    return bestsence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rixyJKdWVVam",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f72318f4-7f4d-4051-9c51-aa5cc239cbe5"
      },
      "source": [
        "lesk('day', 'a day assigned to a particular purpose or observance'.split())"
      ],
      "execution_count": 326,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 326
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IcugYLv1VV56",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e6dd3da7-7b8e-4a33-8bf0-03099c129f92"
      },
      "source": [
        "wn.synsets('day')[2].definition()"
      ],
      "execution_count": 260,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'a day assigned to a particular purpose or observance'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 260
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bfxo_ktFcts3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#corpus_file = files.upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYO7mejVc2-p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus_wsd = []\n",
        "corpus = open('corpus_wsd_50k.txt').read().split('\\n\\n')\n",
        "for sent in corpus:\n",
        "    corpus_wsd.append([s.split('\\t') for s in sent.split('\\n')])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZwgIs_d-kEV",
        "colab_type": "text"
      },
      "source": [
        "Функция вычисления точности работы алгоритма Леска"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJlh9O3Sr5fN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def lesk_acc(corp):\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  for sentence in corp:\n",
        "    norm_sentence = [word[1] for word in sentence]\n",
        "    for word in sentence:\n",
        "      if word[0]: # только многозначные слова\n",
        "        t_word =  word[1] # target word\n",
        "        if wn.synsets(t_word)[lesk(t_word, norm_sentence)] == wn.lemma_from_key(word[0]).synset():\n",
        "          correct += 1\n",
        "        total += 1\n",
        "  acc = correct / total\n",
        "  return acc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XauaNXRts5XR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "1ee1caa5-4a7d-4b2a-db41-dea020b08774"
      },
      "source": [
        "%%time\n",
        "print('Accuracy =', lesk_acc(corpus_wsd[:10000]))"
      ],
      "execution_count": 313,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy = 0.3677168706758914\n",
            "CPU times: user 2min 54s, sys: 3.51 s, total: 2min 58s\n",
            "Wall time: 2min 58s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxePNSTz-uwM",
        "colab_type": "text"
      },
      "source": [
        "Поскольку рассматривались только многозначные слова(от 2 и более значений) - вероятность угадывания должна быть меньше, чем 0.5. Соответственно и результат скорее всего лучше случайного. Для проверки можно написать похожую функцию, которая будет предлагать случайное значение слова."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJVj1xEB_tqh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def random_acc(corp):\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  for sentence in corp:\n",
        "    norm_sentence = [word[1] for word in sentence]\n",
        "    for word in sentence:\n",
        "      if word[0]: # только многозначные слова\n",
        "        t_word =  word[1] # target word\n",
        "        if wn.synsets(t_word)[np.random.choice(len(wn.synsets(t_word)))] == wn.lemma_from_key(word[0]).synset():\n",
        "          correct += 1\n",
        "        total += 1\n",
        "  acc = correct / total\n",
        "  return acc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJeUN7vMA9T_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "94ea3db4-cc3a-4694-c95e-45f0b97a9b7e"
      },
      "source": [
        "%%time\n",
        "print('Random accuracy =', random_acc(corpus_wsd[:10000]))"
      ],
      "execution_count": 335,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Random accuracy = 0.30203299627461416\n",
            "CPU times: user 29 s, sys: 3.14 s, total: 32.1 s\n",
            "Wall time: 32.2 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FoQwC9rzBuzW",
        "colab_type": "text"
      },
      "source": [
        "Таким образом, можно констатировать, что точность для алгоритма Леска лучше, чем случайный подбор возможного значения."
      ]
    }
  ]
}